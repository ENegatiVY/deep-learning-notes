# Structure machine learning project

## week1

### 1.1 why ML
- this course mainly focus on the strategies to  analyzing a machine learning problem
- a lot of experience in building and shipping network

### 1.2 orthogonalization
- during building a network ,we should have different strategy for diffrent problem we might come across

such as

|problem|solution|
| ---------------  | ------------------------ |
|work bad on training set|try diffrent algrithm|
|work bad on dev set| get bigger training set|
|work bad on test set| get bigger dev set|

- this course will focus on waht exactly is the bottleneck to your system's performance

### 1.3 single number evaluation metric
when you have multuple evaluatioins like precision and recall. you need to combine them as one evaluation like F1 score—— harmonic mean of precison and reccall.


### 1.4 satisficing and optimizing metrics
when we have multiple metrics,we can classify them as  optimizing metric and satisficing metric,then we pick the model has the best optimizing metric which satisfies the satisfying metric 
- optimizing metric
- satisficing metric


### 1.5 train/dev/test/distributions
we use develpment set to testify our different ideas, and we use the test set to evaluate how exact our idea works.

dev set and test set should have the same distribution.

### 1.6 size of dev and test set
nowadays, we spend more than 70 percent data to train ,and less data to dev and test
数据集比较小的时候，我们七三开，但是数据集非常大的时候，我们分百分之一给开发集和训练集就足够了

### 1.7 when to change dev/test sets and metrics
evaluation metric cant evaluate the model in the right way,such as one model performs well in your current evaluate metric but it allows adult pictures.
当我们之前规定的评估指标已经无法正确的告诉我们哪个模型更好的时候，我们需要修改评估指标或者训练集。比如误差更小的那个模型无法过滤黄色图片


### 1.8 why human-level performance
the best limit is the bayes error

### 1.9 avoidable bias
bias: the error between training error and bayes error
偏差：训练误差和贝叶斯误差之间的误差
variance: the difference between training error and dev error
方差：训练误差和开发误差之间的差距
we focus on the bigger one when we optimize the model.
我们着重于解决这两个之中更大的那个误差

### 1.10 understand human-level-performance
before we approuch human-level performance it is easy to find out where the problem is the bias or the variance
在接近人类的极限之前，我们很容易判断问题是在偏差还是方差，但是接近之后，就很难判断了，因为我们实际上并不能找到贝叶斯极限在哪里。

### 1.11 surpassing human-level performance
deep learning can easily perform better than human on the porject that have a lot of data,but tend to perform bad on the inception project

机器学习在有大量数据可以访问的项目上，容易表现得比较好，但是在一些比较偏直觉的问题上，则表现得比较差。

### 1.12 imporve your model performance

optimize bias 
- train bigger model
- train longer/better optimizatioin algorithms
- NN architechture/hyperparameters search

variance
- more data
- regularization
	- L2
	- dropout
- NN architechture/hyperparameters search